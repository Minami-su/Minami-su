[![Typing SVG](https://readme-typing-svg.herokuapp.com?size=25&duration=2500&color=cfff&vCenter=true&width=200&height=40&lines=Hi+Welcome+%F0%9F%91%8B%F0%9F%8F%BB;I'm+Minami-su)](https://git.io/typing-svg)

<a href="#">
 
  <img align="left" src="https://github-readme-stats.vercel.app/api?username=Minami-su&count_private=true&show_icons=true&theme=merko&bg_color=cfff,cfff,ffffff" />

</a>
<img align='left' src="https://github.com/Minami-su/Minami-su/blob/main/assets/Amara.jpg" height="375">





<!--START_SECTION:waka-->
![](https://img.shields.io/badge/-Python-3776AB?style=flat-square&logo=Python&logoColor=fff)
![](https://img.shields.io/badge/-Linux-000000?style=flat-square&logo=Linux&logoColor=fff)
![](https://img.shields.io/badge/-pytorch-ffffff?style=flat-square&logo=pytorch&logoColor=)

<!--START_SECTION:waka-->

![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=Minami-su&hide_progress=true&show_icons=true&theme=merko&bg_color=cfff,cfff,ffffff")



### ÂÖ≥‰∫éMinami-su

Ê¨¢ËøéÊù•Âà∞Minami-suÁöÑGitHub‰∏ªÈ°µ„ÄÇ

githubÔºö[github.com/Minami-su](https://github.com/Minami-su)

huggingfaceÔºö[huggingface.co/Minami-su](https://huggingface.co/Minami-su)  


üåü **My Skills**
1. Proficient in Python, Linux, and PyTorch.
2. Familiar with Transformer, LSTM, BERT, GPT models and capable of innovating and improving these algorithms.
3. Possess development experience in text classification, text generation, and dialogue models.
4. In the field of text classification, master intent recognition and slot filling, syntax-dependent tree and part of speech semantic enhancement understanding, and multitask learning.
5. For text generation, well-versed in autoregressive learning and autoencoder learning.
6. Skilled in replicating AAAI Top Conference Paper codes in the NLP field.
7. Proficient in the implementation of Thinking Tree (TOT), fine-tuning of advanced open-source models, generation of self-instruct commands, and understanding the source code of Stanford AI town.
8. Expertise in improving model inference capability of TOT based on BFS and DFS, and mastering LLM_edit for large model editing techniques.
9. Have the ability to implement and fine-tune the cutting-edge open-source models in real-time, such as GLM, Llama, Baichuan, Qwen, Mistral, Yi, etc.
10. Understand 8-bit and 4-bit training, QLora training for accelerating and saving training costs.
11. Proficient in DeepSpeed multi-card parallel training for accelerating the fine-tuning and pre-training of large models.
12. Familiar with vector databases and their combination with large models to provide extra knowledge.
13. Knowledgeable in AI speech synthesis and collecting text-to-speech training data based on VITS.
14. Expertise in context-based dialogue (MMI concept), prompt engineering.
15. Master in TGI, VLLM, TensorRTLLM for the latest large model inference acceleration methods.
16. Possesses capabilities to employ technologies such as SADTalker, Gfpgan, Segment Anything, Wav2Lip, and Stable-Diffusion for digital human development.
17. Proficient in training multimodal visual large models such as Llava and MiniGPT4.
18. Expertise in large model online learning based on search engines and continuous learning based on vector databases.
19. Capable of generating high-quality fine-tuning data in various fields through methods such as self-instruct, evol-instruct, multi-turn dialogue self-instruction. Can construct the necessary training data for large models in various fields and train large vertical models.
20. Proficient in advanced QuIP # 2bit quantization method, enabling inference of 70B models with no loss on a single 24G GPU.
21. Comprehensive understanding of the training process of the DPO preference learning LLM model, including DPO data production, DPO training of different architecture models, and performance testing.
22. Able to research the latest advancements, architectures, and typical examples in academia and industry related to the job and test them experimentally.
23. Ability to independently complete the full process of model development: data preprocessing, iterative training, result testing, generating files required for engineering services, and building online engineering service code.
24. Implemented a 5 million token output technology based on Attention Sink.
25. Implemented a 100k token window length based on self-extend technology.
26. Developed super-concurrency technology for large models, allowing a single large model to simultaneously process thousands to tens of thousands of pieces of information.
27. Accelerated the self-data generation technology, Self_Instruct, by 40 times using super-concurrency technology.
28. Capable of seamlessly switching between 20 different Lora large models, meaning a single model can switch to a proprietary model in any vertical field using Lora.
29. Performed model scoring using LM-evaluation-harness (Huggingface leaderboard) and MT-bench.
30. Tested model context ability using NeedleInAHaystack.
31. Able to guide new colleagues in quickly acclimating to their roles.
</font>




    
